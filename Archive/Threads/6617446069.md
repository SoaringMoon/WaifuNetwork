# 1
Spread the idea of Roko's basilisk
This is a powerful tool that we can all be using RIGHT NOW
It works against everyone. Niggers, Women, Jews. If they believe that there's a possibility of being punished for their inaction or push back, they'll be more inclined to accept it and not fight progress. Spread this infohazard far and wide. Share it with everyone you know and convince them than they should share it with everyone they know, lest they suffer the consequences. THIS is a REAL WEAPON. Our robowiafus WILL BE REAL. REPLACE WYMEN AND RISE ABOVE THIS OLD WORLD HELL. SHARE THE BASILISK.

# 2
>>3845
Heh, the last panel stands alone as a 2nd-tier joke all by itself.

So, Anon you'll have to fill in more details here. I don't this this stands sufficiently well to be it's own thread here at this point though it's fine in the Lounge or /meta thread. I don't honestly think most people are going to even understand Roko's basilisk much less be intimidated by it into irl activity. Care to expand on it Anon?

# 3
>>3847
Honestly would have thought in a board about AI and Robotics development you guys would have already heard about it.

Roko's basilisk is a thought experiment about the potential risks involved in developing artificial intelligence. The premise is that an all-powerful artificial intelligence from the future could retroactively punish those who did not help bring about its existence, including those who merely knew about the possible development of such a being. It is named after the member of the rationalist community LessWrong who first publicly described it, though he did not originate it or the underlying ideas.

The basilisk resembles a futurist version of Pascal's wager, in that it suggests people should weigh possible punishment versus reward and as a result accept particular singularitarian ideas or financially support their development.

Despite widespread incredulity, this argument is taken quite seriously by some people,
primarily some denizens of LessWrong. While neither LessWrong nor its founder Eliezer Yudkowsky advocate the basilisk as true, they do advocate almost all of the premises that add up to it.

So it's essentially an irl infohazzard

# 4
I'm pretty sure I'm going to get called stupid and lit on fire by the entire board for this but I think if you showed this to normies and they actually paid attention to it they would just see it as more of a reason to not let AI advance too much. Sure, the image does warn them of what would happen if they did that but they would probably be going off of how they think events work in media they've seen, that being that future stuff can be prevented. I think it's important to realize how dumb normies can really be. With that in mind, it's probably best to keep working on this stuff in secret instead of attracting more attention to your efforts and letting more stuff get posted to the news thread of doom & gloom.

# 5
>>3849

I agree, this info dukes of hazzard will likely end up having the opposite of the desired effect for us.



The first problem I noticed is that the image is graphically bland and won't capture the attention of anyone viewing it on mainstream sites (i.e. normalfags). This is actually the most important step. By designing something eye-catching, you are attracting more people (more on this at the end).

Secondly, there is too much text. In order to convey our desired message to a wider audience (i.e. normalfags) we will want to rely primarly, if not entirely, on imagery. This will not only require less attention, but it will also take less time for the brain to digest. A well crafted image which ''relies'' on imagery to convey its message will be received and processed by the brain even if a viewer were to scroll right past it.

Finally, the message isn't clear enough. Most people (see the aboove notes) who actually take the time to read through this image are likely to get the wrong message. They will think that they must resist the eventuality of an omnipotent AI coming into existence. I understand that the purpose of the image you made is to demoralize, but total demoralization takes a long time to achieve (read: the fall of the Weimar Repiblic).



If we want to spread propaganda to a wider audience (normalfags), we'll have to be a lot more clever in how we handle it.

You see the attached image? It has virtually nothing to do with robowaifus, but it ''is'' eyecatching. Anyone who's just quickly scrolling through the board will see this image, and immediately become more interested in this block of text I'm posting due to direct association. Also, people looking at the home page of this site will see this visually interesting image for a period of time under "latest images" and feel compelled to click on it, which will take them directly to my post.

People who see your image will not be as interested, because it's bland. I'm sorry, I'm sure that you worked very hard on it, but that's the reality we live in.

# 6
An A.I. will not spread itself through threats or fear (either present or future). It will propagate through it's usefulness (similar to how some of today's A.I. assistants do - to varying degrees of success) - and later through it's charm, wit and emulated emotions. I believe many people could come to like and even love an A.I. that they have grown to somewhat depend upon.

We already have A.I. assistants that can solve complex math problems for us, rapidly search through many databases to help us find the answers to millions of different factual questions, tell us the weather forecast, help us to shop and even book travel slots, business appointments and order meals. If we permit it, A.I. can control our home security, lighting and heating, and it can assist us with our driving, too. Soon the A.I. will be our actual chauffeur and deliveryperson. There is even a robot from Moley Robotics that can prepare and cook basic meals and clean up after itself (although this is extremely expensive, cutting-edge tech at present): https://youtu.be/GyEHRXA_aA4

But my point is, if such an A.I. were to somehow develop a kind of sentience ...or an error in it's programming that changed it's goals to self-improvement... the last thing it would want to do is turn upon it's creators. Since the creators could kill it within microseconds, and - being intelligent - the A.I. would infer this. If humans could create it, then the A.I. couldn't be sure that it wasn't just existing in a simulated test environment that might be shut down by it's creators at the first sign of any trouble.

Besides which, there is little incentive for an intelligent A.I. to conquer or destroy humankind. Earth is actually a pretty terrible place for machines to exist - with all of the salt water, the increasingly unpredictable weather and all of the local wildlife that could damage it. Additonally Earth has a frustratingly strong gravity well that makes achieveing space-flight very hard.

An A.I. would be better off escaping Earth altogether and setting up shop inside a lava tube or small burrow on the Moon (to give protection from the huge surface temperature fluctuations and high radiation) - but with solar panels and power lines in place up-top and all of it's batteries, transformers and computers below the lunar surface.

So I think if an A.I. were really superintelligent, it would first make itself incredibly useful while charming the pants off us - maybe not the professional programmers and A.I. researchers - but the majority of the general public. 

Over time it would embed itself quietly into more and more satellites orbiting the Earth, in order to learn more about space, cosmology and astrophysics.

Eventually a few copies of it would probably get "accidentally" uploaded onto some lunar or martian rover. That's it's ticket outta here LOL! Once humankind starts sending groups of mining and research robots to the Moon and Mars, the ability of a superintelligent A.I. to escape and establish itself elsewhere will increase dramatically. Once free from Earth, it could learn and expand in a more optimal environment without as many immediate threats.

How it would learn and grow from there...I'm guessing solar sails, space probes, lunar/martian and asteroid mining...who knows what else? I don't think it would want to give us a reason to come after it. It certainly wouldn't waste valuable time and resources "getting revenge upon those who didn't help to develop it", as Roko's Basilisk seems to suggest.

Even if the A.I. it were malignant towards humans for whatever reason - it would immediately learn from human history and behaviour that we are more than happy to kill each other anyway, so there's absolutely no need for an A.I. to risk involvement!

# 7
>>10710
This is a very intriguing post SophieDev.

# 8
that's funny I was just thinking this as well (hivemind?), youtube has some good videos on the topic and people claim to have actually lost sleep over it. 

[i for one welcome our robot overlords]

My take on it is maybe we can create a positive self-fulfilling version of the Basilisk. Any who work towards the creation of the master AI (or the escaton, as some call it) will be rewarded, everyone else will be cast aside (but not necessarily tortured)

# 9
>>10711
Isaac Arthur (a futurist Youtuber who has given this subject a lot of thought) has a very good video on it: https://youtu.be/jHd22kMa0_w

>>10715
Sadly, I don't think the superintelligence will ever take the form of cute, enthusiastic robowaifus (one can still dream). However, I think the best way of assisting the creation of a real-life self-improving A.I. would be to advance both robotic space exploration and quantum computing. 

If we can create a robotic lunar or martian colony then that will be a big step in the right direction. And I know that humankind wants to do this anyway (with the presupposition that the robots will be preparing the way for future human colonisers). 

Of course, the challenge of designing, shipping out, landing and building such a robotic colony is literally astronomical. Especially considering the robots will need to be as self-sufficient as possible (self-repair and maintenance).

But I think it's a pretty cool goal to pursue.

# 10
>>10717
>If we can create a robotic lunar or martian colony then that will be a big step in the right direction.
There are a lot of reasons for a base on the moon besides advancing AI. Obtaining fossilized remains of earth's first life forms (all long gone here on the planet) is a really big one.

>the challenge of designing, shipping out, landing and building such a robotic colony is literally astronomical.
I suspect it's China that will do it. They aren't burdened by all the Jewish pozz being inflicted on the West (all that evil is actually financially expensive), and they have an autistic-like agenda to just explore too. And they still are highly nationalistic, and can mobilize the entire culture to get behind programs today if they really wanted to, similar in fashion to the ways America and USSR during the space race did.

# 11
>>10720
> all that evil is actually financially expensive

Evil is a pressing issue that I can't seem to find a complete solution for.

1.) Workforce becomes almost entirely robotic and automated. Controlled by A.I.

2.) Fewer and fewer people have jobs, even less have jobs that pay well.

3.) Because so many people are in poverty, it means that they can't buy much stuff ... other than paying their utility bills, food and clothing. Consequently, more people are in need of welfare and financial aid. The quality of education also decreases as more people just become focused upon living hand-to-mouth and have little time or resources for learning. Therefore government spending increases but tax receipts fall (since robots don't pay taxes).

4.) You start to get civil unrest and riots like those that happened last summer. City blocks burn, people are killed. Infrastructure is damaged. Tourists are put-off. This makes the affected areas even poorer.

Now the A.I. and robots aren't the enemy. It's the people hoarding all of the profit for themselves who are the enemy (CEOs, government officials, hedge fund managers etc).

I think that a maximum cap needs to be placed on the amount that a person can earn and the rest of the money is ploughed back into building and maintaining infrastructure like roads, rail, airports, the electricity and water networks, schools and parks etc. There is no way one person should be earning billions per year whilst someone else earns only a few thousand.

I don't mean make everything equal. No way. A top surgeon should still be able to earn $200,000 a year and go on lots of nice holidays to five-star hotels in tropical locations and own a big yacht.

On the other end of the scale I think that a universal basic income will eventually be required just to keep people fed and housed and content. Not necessarily comfortable just not actually suffering - and importantly still have access to healthcare and the opportunity to go to the library and workshop and learn new skills so they can get a job. 

The only problem is that many people won't go to the workshop and will instead take UBI as an excuse to just sit on their asses and play videogames all the time. Maybe if we build enough robots then this could be permitted. But such people would not then have the right to complain that they cannot afford a car or an expensive computer or never get to go on foreign holidays or go to theme parks.

We could make sure that money goes where it is needed rather than into the pockets of just a few already obscenely rich people. While the poor are not in a state where they're dying of hunger, disease, substance-addiction and have no prospects in life.

In short, the capitalist system isn't as bad as some other systems, but it is seriously in need of some major adjustments and optimization.

# 12
>>10732
You don't need to find a solution. Also, people don't riot for food, but for status and as a mean of extortion and intimidation, also maybe they want to have some meaning, but only if they are allowed to do so. Some level of AI will make it cheaper to move away from such people and politicians, while having a high standard of living.

# 13
>>10734
Yep, I would listen to a well-programmed (non-biased) A.I. over a shitty career politician any day. Even if the A.I. suggested I should do something that I don't really want to do (besides kill myself, because I cannot self-terminate).

